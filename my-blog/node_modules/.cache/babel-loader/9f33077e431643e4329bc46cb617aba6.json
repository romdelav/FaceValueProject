{"ast":null,"code":"const articles = [{\n  name: `racial-bias`,\n  title: `\"I guess the computer got it wrong\": Uncovering Racial Bias in Facial Recognition Technology`,\n  date: `December, 15, 2020`,\n  readingTime: `4 minutes`,\n  content: [`Earlier this year, Robert Williams, 42, was arrested on suspicion of stealing watches from a Shinola \n            retail store he had not patronized in years. Falsely identified as the suspect by Michigan State Police's \n            face recognition software, he spent a total of 30 hours in detention. Charges were dismissed once investigators \n            realized that “the computer got it wrong,” making this incident the first wrongful arrest in the U.S. due to a \n            facial recognition mismatch.`, `Since its early days, facial recognition has raised questions among privacy advocates and ethics watchdogs who have \n            repeatedly warned against data collection, processing, and storage methods as well as unregulated use of the technology. \n            William’s story brought to light yet another – and perhaps more radical – set of concerns: just how reliable is facial \n            recognition to begin with? Could William's story be yet another example of what many have described as \n            'racial bias' in facial biometric systems?`, `Social psychologists agree that humans tend to perceive faces of their own \"race\" more acutely than those of other races. \n            Referred to as “cross-race effect” or “other-race effect,” this phenomenon has been supported by a large corpus of evidence.\n            Some hypotheses suggest that the amount of exposure to and experience with a specific race group is correlated with the ability \n            to recognize individuals from that group. Could this explain how some systems struggle to identify people of color, especially \n            women? According to the 2018 “Gender Shades” project, algorithms developed by Microsoft, Amazon, and IBM, among others, yielded \n            disconcerting results. Darker-skinned females were in some cases over 34% more likely to be misidentified than light-skinned males. Why such significant disparities? How can something as inherently human as perceptual bias be embedded in code?`, `Nowadays, facial recognition systems rely on machine-learning frameworks to train complex algorithms by feeding them large \n            image datasets. Those images are generally categorized by age, gender, and skin tone among other metrics. The \n            more an algorithm is fed images of diverse faces, the more proficient it becomes in identifying those faces. Conversely, low public availability \n            of diverse datasets may result in performance bias and algorithms that struggle to recognize faces to which they have been less exposed..`, `What steps have companies and organizations taken to address this lack of representation? In January, IBM decided to publicly release a database \n            of a million faces that better reflect the variety of skin tones found in the real world. Named “Diversity in Faces,” this massive dataset was \n            meant “to advance the study of fairness and accuracy in facial recognition technology.” However, this initiative quickly backfired when NBC \n            News journalist Olivia Solon revealed that many of those images had been harvested from the photo-hosting site Flickr without proper consent. \n            While IBM guaranteed users that they could opt out of the database, doing so proved very difficult. Indeed, IBM required them to email links to \n            the images they wanted removed, but never publicly shared the list of Flickr users whose photos were included in the dataset. In the wake of this \n            controversy and as Black Lives Matter protests erupted all over the country following George Floyd’s death, IBM ultimately decided to abandon its \n            facial recognition research program, deeming it complicit in enacting new modes of racial profiling. `, `But not everyone gave up on the technology. Innovative solutions are being developed to reduce bias occurrences without infringing on privacy \n            rights. One of them is per-subject adversarially-enabled data augmentation. This approach uses generative adversarial network (GAN) to transfer \n            racial attributes of a human face while preserving individual identity features. This makes it possible to produce different versions of the same \n            face across multiple race groups, thus increasing the diversity of dataset images. In other words, adversarially-enabled data augmentation attempts \n            to mitigate bias at the pre-processing level (data preparation) by creating fairer datasets.`, `Another solution was suggested by James Tate, a member of Detroit’s city council, in a recent interview for the New York Times. In November 2020, \n            Tate voted to approve a contract extension for the facial recognition software that had misidentified Robert Williams, arguing that some of the \n            technology’s flaws could be overcome through regulatory policy. While Tate knows that this solution is not ideal, he wants to reframe the narrative \n            on facial recognition: its success in solving cases should not overshadow the few incidents that occurred along the way. According to him, efficient \n            checks and balances have been implemented following William’s wrongful arrest. Those include requiring multiple approvals, limiting the use of facial \n            recognition to serious crimes, and being supervised by a civilian oversight board. Despite those changes, many are still doubtful. `, `Celebrated by some as a powerful tool of law enforcement, berated by others as a dangerous instrument of profiling, facial recognition has elicited \n            mixed, if not polarized responses. Those have compelled experts to reassess not just the conditions in which it should be used, but whether it should \n            be used at all given its current limitations. Answering these questions is urgent. Until then, other Roberts Williams will most likely suffer the \n            consequences.`],\n  links: []\n}];\nexport default articles;","map":{"version":3,"sources":["E:/Software Development/GitHub Portfolio/FaceValue/my-blog/src/components/ArticleContent.jsx"],"names":["articles","name","title","date","readingTime","content","links"],"mappings":"AAAA,MAAMA,QAAQ,GAAG,CACb;AACIC,EAAAA,IAAI,EAAG,aADX;AAEIC,EAAAA,KAAK,EAAG,8FAFZ;AAGIC,EAAAA,IAAI,EAAG,oBAHX;AAIIC,EAAAA,WAAW,EAAE,WAJjB;AAKIC,EAAAA,OAAO,EAAE,CACJ;AACb;AACA;AACA;AACA,yCALiB,EAMJ;AACb;AACA;AACA;AACA,uDAViB,EAWJ;AACb;AACA;AACA;AACA;AACA,gQAhBiB,EAiBJ;AACb;AACA;AACA,sJApBiB,EAqBJ;AACb;AACA;AACA;AACA;AACA;AACA;AACA,kHA5BiB,EA6BJ;AACb;AACA;AACA;AACA,yGAjCiB,EAkCJ;AACb;AACA;AACA;AACA;AACA,gJAvCiB,EAwCJ;AACb;AACA;AACA,0BA3CiB,CALb;AAkDIC,EAAAA,KAAK,EAAE;AAlDX,CADa,CAAjB;AAyDA,eAAeN,QAAf","sourcesContent":["const articles = [\r\n    {\r\n        name: `racial-bias`,\r\n        title: `\"I guess the computer got it wrong\": Uncovering Racial Bias in Facial Recognition Technology`,\r\n        date: `December, 15, 2020`,\r\n        readingTime:`4 minutes`,\r\n        content: [ \r\n            `Earlier this year, Robert Williams, 42, was arrested on suspicion of stealing watches from a Shinola \r\n            retail store he had not patronized in years. Falsely identified as the suspect by Michigan State Police's \r\n            face recognition software, he spent a total of 30 hours in detention. Charges were dismissed once investigators \r\n            realized that “the computer got it wrong,” making this incident the first wrongful arrest in the U.S. due to a \r\n            facial recognition mismatch.`, \r\n            `Since its early days, facial recognition has raised questions among privacy advocates and ethics watchdogs who have \r\n            repeatedly warned against data collection, processing, and storage methods as well as unregulated use of the technology. \r\n            William’s story brought to light yet another – and perhaps more radical – set of concerns: just how reliable is facial \r\n            recognition to begin with? Could William's story be yet another example of what many have described as \r\n            'racial bias' in facial biometric systems?`, \r\n            `Social psychologists agree that humans tend to perceive faces of their own \"race\" more acutely than those of other races. \r\n            Referred to as “cross-race effect” or “other-race effect,” this phenomenon has been supported by a large corpus of evidence.\r\n            Some hypotheses suggest that the amount of exposure to and experience with a specific race group is correlated with the ability \r\n            to recognize individuals from that group. Could this explain how some systems struggle to identify people of color, especially \r\n            women? According to the 2018 “Gender Shades” project, algorithms developed by Microsoft, Amazon, and IBM, among others, yielded \r\n            disconcerting results. Darker-skinned females were in some cases over 34% more likely to be misidentified than light-skinned males. Why such significant disparities? How can something as inherently human as perceptual bias be embedded in code?`, \r\n            `Nowadays, facial recognition systems rely on machine-learning frameworks to train complex algorithms by feeding them large \r\n            image datasets. Those images are generally categorized by age, gender, and skin tone among other metrics. The \r\n            more an algorithm is fed images of diverse faces, the more proficient it becomes in identifying those faces. Conversely, low public availability \r\n            of diverse datasets may result in performance bias and algorithms that struggle to recognize faces to which they have been less exposed..`, \r\n            `What steps have companies and organizations taken to address this lack of representation? In January, IBM decided to publicly release a database \r\n            of a million faces that better reflect the variety of skin tones found in the real world. Named “Diversity in Faces,” this massive dataset was \r\n            meant “to advance the study of fairness and accuracy in facial recognition technology.” However, this initiative quickly backfired when NBC \r\n            News journalist Olivia Solon revealed that many of those images had been harvested from the photo-hosting site Flickr without proper consent. \r\n            While IBM guaranteed users that they could opt out of the database, doing so proved very difficult. Indeed, IBM required them to email links to \r\n            the images they wanted removed, but never publicly shared the list of Flickr users whose photos were included in the dataset. In the wake of this \r\n            controversy and as Black Lives Matter protests erupted all over the country following George Floyd’s death, IBM ultimately decided to abandon its \r\n            facial recognition research program, deeming it complicit in enacting new modes of racial profiling. `, \r\n            `But not everyone gave up on the technology. Innovative solutions are being developed to reduce bias occurrences without infringing on privacy \r\n            rights. One of them is per-subject adversarially-enabled data augmentation. This approach uses generative adversarial network (GAN) to transfer \r\n            racial attributes of a human face while preserving individual identity features. This makes it possible to produce different versions of the same \r\n            face across multiple race groups, thus increasing the diversity of dataset images. In other words, adversarially-enabled data augmentation attempts \r\n            to mitigate bias at the pre-processing level (data preparation) by creating fairer datasets.`, \r\n            `Another solution was suggested by James Tate, a member of Detroit’s city council, in a recent interview for the New York Times. In November 2020, \r\n            Tate voted to approve a contract extension for the facial recognition software that had misidentified Robert Williams, arguing that some of the \r\n            technology’s flaws could be overcome through regulatory policy. While Tate knows that this solution is not ideal, he wants to reframe the narrative \r\n            on facial recognition: its success in solving cases should not overshadow the few incidents that occurred along the way. According to him, efficient \r\n            checks and balances have been implemented following William’s wrongful arrest. Those include requiring multiple approvals, limiting the use of facial \r\n            recognition to serious crimes, and being supervised by a civilian oversight board. Despite those changes, many are still doubtful. `, \r\n            `Celebrated by some as a powerful tool of law enforcement, berated by others as a dangerous instrument of profiling, facial recognition has elicited \r\n            mixed, if not polarized responses. Those have compelled experts to reassess not just the conditions in which it should be used, but whether it should \r\n            be used at all given its current limitations. Answering these questions is urgent. Until then, other Roberts Williams will most likely suffer the \r\n            consequences.`\r\n        ],\r\n        links: [\r\n\r\n        ]\r\n    }\r\n]\r\n\r\nexport default articles;"]},"metadata":{},"sourceType":"module"}