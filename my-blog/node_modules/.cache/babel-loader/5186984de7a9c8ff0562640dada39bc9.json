{"ast":null,"code":"import { jsxDEV as _jsxDEV } from \"react/jsx-dev-runtime\";\nimport { Fragment as _Fragment } from \"react/jsx-dev-runtime\";\nvar _jsxFileName = \"E:\\\\Software Development\\\\SD744-FinalProject\\\\my-blog\\\\src\\\\pages\\\\HomePage.jsx\";\nimport React from 'react';\nimport img from '../img/facialrecognition.jpg';\nimport figure1 from '../img/Figure1.jpg';\nimport figure2 from '../img/Figure2.jpg';\nimport figure3 from '../img/Figure3.jpg';\n\nconst HomePage = () => /*#__PURE__*/_jsxDEV(_Fragment, {\n  children: /*#__PURE__*/_jsxDEV(\"div\", {\n    className: \"grid-container\",\n    children: [/*#__PURE__*/_jsxDEV(\"div\", {\n      className: \"cell1\",\n      children: [/*#__PURE__*/_jsxDEV(\"div\", {\n        className: \"title1\",\n        children: \"\\\"I guess the computer got it wrong\\\"\"\n      }, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 11,\n        columnNumber: 13\n      }, this), /*#__PURE__*/_jsxDEV(\"div\", {\n        children: \"Uncovering Racial Bias in Facial Recognition Technology\"\n      }, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 12,\n        columnNumber: 13\n      }, this)]\n    }, void 0, true, {\n      fileName: _jsxFileName,\n      lineNumber: 10,\n      columnNumber: 9\n    }, this), /*#__PURE__*/_jsxDEV(\"div\", {\n      className: \"cell2\",\n      children: /*#__PURE__*/_jsxDEV(\"img\", {\n        src: img,\n        alt: \"cannot display\",\n        width: \"100%\"\n      }, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 15,\n        columnNumber: 13\n      }, this)\n    }, void 0, false, {\n      fileName: _jsxFileName,\n      lineNumber: 14,\n      columnNumber: 9\n    }, this), /*#__PURE__*/_jsxDEV(\"div\", {\n      className: \"cell3\",\n      children: [/*#__PURE__*/_jsxDEV(\"div\", {\n        className: \"border-bttm\",\n        children: [/*#__PURE__*/_jsxDEV(\"h3\", {\n          children: \"Romain Delaville\"\n        }, void 0, false, {\n          fileName: _jsxFileName,\n          lineNumber: 19,\n          columnNumber: 17\n        }, this), /*#__PURE__*/_jsxDEV(\"div\", {\n          className: \"date\",\n          children: \"DEC 15, 2020\"\n        }, void 0, false, {\n          fileName: _jsxFileName,\n          lineNumber: 20,\n          columnNumber: 17\n        }, this)]\n      }, void 0, true, {\n        fileName: _jsxFileName,\n        lineNumber: 18,\n        columnNumber: 13\n      }, this), /*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 22,\n        columnNumber: 13\n      }, this), /*#__PURE__*/_jsxDEV(\"p\", {\n        children: [\"Earlier this year, Robert Williams, 42, was arrested on suspicion of stealing watches from a trendy Shinola retail store he had not patronized in years. Falsely identified as the suspect by Michigan State Police's face recognition software, he spent a total of 30 hours in detention. Charges were dismissed once investigators realized that \\u201Cthe computer got it wrong,\\u201D\", /*#__PURE__*/_jsxDEV(\"a\", {\n          className: \"referenceLink\",\n          href: \"#1\",\n          children: \"[1]\"\n        }, void 0, false, {\n          fileName: _jsxFileName,\n          lineNumber: 26,\n          columnNumber: 59\n        }, this), \" making this incident the first wrongful arrest in the U.S. due to a facial recognition mismatch.\"]\n      }, void 0, true, {\n        fileName: _jsxFileName,\n        lineNumber: 23,\n        columnNumber: 13\n      }, this), /*#__PURE__*/_jsxDEV(\"p\", {\n        children: [\"Since its early days, facial recognition has raised questions among privacy advocates and ethics watchdogs who have repeatedly warned against data collection, processing, and storage methods as well as unregulated use of the technology. William\\u2019s story brought to light yet another \\u2013 and perhaps more radical \\u2013 set of concerns: just how reliable is facial recognition to begin with? Can it be trusted at all? Last year, some scholars had already exposed what they describe as racial bias in facial biometric systems.\", /*#__PURE__*/_jsxDEV(\"a\", {\n          className: \"referenceLink\",\n          href: \"#2\",\n          children: \"[2]\"\n        }, void 0, false, {\n          fileName: _jsxFileName,\n          lineNumber: 33,\n          columnNumber: 57\n        }, this), /*#__PURE__*/_jsxDEV(\"a\", {\n          className: \"referenceLink\",\n          href: \"#3\",\n          children: \"[3]\"\n        }, void 0, false, {\n          fileName: _jsxFileName,\n          lineNumber: 33,\n          columnNumber: 103\n        }, this), \" In what follows, I will examine this issue in the context of its use by law enforcement. I will draw on a corpus of tweets, newspaper articles, and scholarly essays made available by the Twitter API to unpack this notion of racial bias, identify its origins, and discuss recent efforts to correct it. Throughout this essay, I will be using the term \\u201Crace\\u201D instead of \\u201Cethnicity\\u201D to align with the terminology adopted in academic studies.\"]\n      }, void 0, true, {\n        fileName: _jsxFileName,\n        lineNumber: 29,\n        columnNumber: 13\n      }, this), /*#__PURE__*/_jsxDEV(\"p\", {\n        children: [\"Social psychologists agree that humans tend to perceive faces of their own race more acutely than those of other races. Referred to as \\u201Ccross-race effect\\u201D or \\u201Cother-race effect,\\u201D this phenomenon has been supported by a large corpus of evidence.\", /*#__PURE__*/_jsxDEV(\"a\", {\n          className: \"referenceLink\",\n          href: \"#4\",\n          children: \"[4]\"\n        }, void 0, false, {\n          fileName: _jsxFileName,\n          lineNumber: 41,\n          columnNumber: 17\n        }, this), /*#__PURE__*/_jsxDEV(\"a\", {\n          className: \"referenceLink\",\n          href: \"#5\",\n          children: \"[5]\"\n        }, void 0, false, {\n          fileName: _jsxFileName,\n          lineNumber: 41,\n          columnNumber: 63\n        }, this), \" Some hypotheses suggest that the amount of exposure to and experience with a specific race group is correlated with the ability to recognize individuals from that group. This could explain how some systems struggle to identify people of color, especially women. According to the 2018 \\u201CGender Shades\\u201D project, algorithms developed by Microsoft, Amazon, and IBM, among others, yielded disconcerting results. Darker-skinned women were over 34% more likely to be misidentified than light-skinned males.\", /*#__PURE__*/_jsxDEV(\"a\", {\n          className: \"referenceLink\",\n          href: \"#6\",\n          children: \"[6]\"\n        }, void 0, false, {\n          fileName: _jsxFileName,\n          lineNumber: 45,\n          columnNumber: 109\n        }, this), \" This begs the question: how can something as inherently human as perceptual bias be embedded in code?\"]\n      }, void 0, true, {\n        fileName: _jsxFileName,\n        lineNumber: 39,\n        columnNumber: 13\n      }, this), /*#__PURE__*/_jsxDEV(\"p\", {\n        children: [/*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n          fileName: _jsxFileName,\n          lineNumber: 49,\n          columnNumber: 17\n        }, this), /*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n          fileName: _jsxFileName,\n          lineNumber: 49,\n          columnNumber: 22\n        }, this), /*#__PURE__*/_jsxDEV(\"img\", {\n          src: figure3,\n          alt: \"cannot display\",\n          className: \"figure3\"\n        }, void 0, false, {\n          fileName: _jsxFileName,\n          lineNumber: 50,\n          columnNumber: 17\n        }, this), /*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n          fileName: _jsxFileName,\n          lineNumber: 51,\n          columnNumber: 17\n        }, this), /*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n          fileName: _jsxFileName,\n          lineNumber: 51,\n          columnNumber: 22\n        }, this)]\n      }, void 0, true, {\n        fileName: _jsxFileName,\n        lineNumber: 48,\n        columnNumber: 13\n      }, this), /*#__PURE__*/_jsxDEV(\"p\", {\n        children: \"Nowadays, facial recognition systems rely on machine-learning frameworks to train complex algorithms by feeding them large image datasets. Those images are generally categorized by age, gender, skin tone, and a variety of other metrics. The more an algorithm is fed images of diverse faces, the more proficient it becomes in identifying those faces. One possible explanation for the racial bias of facial recognition systems may thus have to do with low public availability of diverse datasets \\u2013 those are indeed quite rare. A consequence of this shortage is to encode the other-race effect directly into the software application layer as algorithms struggle to recognize faces that they have been less exposed to.\"\n      }, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 53,\n        columnNumber: 13\n      }, this), /*#__PURE__*/_jsxDEV(\"p\", {\n        children: [\"If performance bias does stem from a lack of representation in data, then what are companies and organizations doing to address it? In January, IBM decided to publicly release a database of a million faces that better reflect the variety of skin tones found in the real world. Named \\u201CDiversity in Faces,\\u201D this massive dataset was meant \\u201Cto advance the study of fairness and accuracy in facial recognition technology.\\u201D\", /*#__PURE__*/_jsxDEV(\"a\", {\n          className: \"referenceLink\",\n          href: \"#7\",\n          children: \"[7]\"\n        }, void 0, false, {\n          fileName: _jsxFileName,\n          lineNumber: 63,\n          columnNumber: 76\n        }, this), \" However, IBM\\u2019s initiative quickly backfired when NBC News journalist Olivia Solon revealed that many of those images were collected from the photo-hosting site Flickr without consent. While IBM guaranteed users that they could opt out of the database, doing so proved very difficult. Indeed, IBM required them to email links to the images they wanted removed, but never publicly shared the list of Flickr users whose photos were included in the dataset.\", /*#__PURE__*/_jsxDEV(\"a\", {\n          className: \"referenceLink\",\n          href: \"#8\",\n          children: \"[8]\"\n        }, void 0, false, {\n          fileName: _jsxFileName,\n          lineNumber: 67,\n          columnNumber: 100\n        }, this), \" In the wake of this controversy and as Black Lives Matter protests erupted all over the country following George Floyd\\u2019s death, IBM ultimately decided to abandon its facial recognition research program, implying that the only way to get rid of facial recognition bias was to get rid of facial recognition altogether.\"]\n      }, void 0, true, {\n        fileName: _jsxFileName,\n        lineNumber: 60,\n        columnNumber: 13\n      }, this), /*#__PURE__*/_jsxDEV(\"p\", {\n        children: [/*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n          fileName: _jsxFileName,\n          lineNumber: 73,\n          columnNumber: 17\n        }, this), /*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n          fileName: _jsxFileName,\n          lineNumber: 73,\n          columnNumber: 22\n        }, this), /*#__PURE__*/_jsxDEV(\"img\", {\n          src: figure1,\n          alt: \"cannot display\",\n          className: \"figure1\"\n        }, void 0, false, {\n          fileName: _jsxFileName,\n          lineNumber: 74,\n          columnNumber: 17\n        }, this), /*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n          fileName: _jsxFileName,\n          lineNumber: 75,\n          columnNumber: 17\n        }, this), /*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n          fileName: _jsxFileName,\n          lineNumber: 75,\n          columnNumber: 22\n        }, this)]\n      }, void 0, true, {\n        fileName: _jsxFileName,\n        lineNumber: 72,\n        columnNumber: 13\n      }, this), /*#__PURE__*/_jsxDEV(\"p\", {\n        children: [\"But not everyone gave up on improving the technology. In March of this year, a group of computer scientists and engineers at the University of Durham, England, proposed an innovative method to reduce bias occurrences without infringing on privacy rights: per-subject adversarially-enabled data augmentation.\", /*#__PURE__*/_jsxDEV(\"a\", {\n          className: \"referenceLink\",\n          href: \"#9\",\n          children: \"[9]\"\n        }, void 0, false, {\n          fileName: _jsxFileName,\n          lineNumber: 79,\n          columnNumber: 77\n        }, this), \" This approach uses generative adversarial network (GAN) to transfer racial attributes of a human face while preserving individual identity features. This makes it possible to produce different versions of the same face across multiple race groups, thus increasing the diversity of dataset images. In other words, adversarially-enabled data augmentation attempt to mitigate bias at the pre-processing level (data preparation) by creating fair datasets. While accuracy gains were made, they seemed minimal and only went up by 1%. Could it be that GAN, a fairly new technology that was developed in 2014, may itself need further correction?\"]\n      }, void 0, true, {\n        fileName: _jsxFileName,\n        lineNumber: 77,\n        columnNumber: 13\n      }, this), /*#__PURE__*/_jsxDEV(\"p\", {\n        children: [/*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n          fileName: _jsxFileName,\n          lineNumber: 87,\n          columnNumber: 17\n        }, this), /*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n          fileName: _jsxFileName,\n          lineNumber: 87,\n          columnNumber: 22\n        }, this), /*#__PURE__*/_jsxDEV(\"img\", {\n          src: figure2,\n          alt: \"cannot display\",\n          width: \"100%\"\n        }, void 0, false, {\n          fileName: _jsxFileName,\n          lineNumber: 88,\n          columnNumber: 17\n        }, this), /*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n          fileName: _jsxFileName,\n          lineNumber: 89,\n          columnNumber: 17\n        }, this), /*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n          fileName: _jsxFileName,\n          lineNumber: 89,\n          columnNumber: 22\n        }, this)]\n      }, void 0, true, {\n        fileName: _jsxFileName,\n        lineNumber: 86,\n        columnNumber: 13\n      }, this), /*#__PURE__*/_jsxDEV(\"p\", {\n        children: [\"A third way to solve some of the issues that have plagued facial recognition was suggested by James Tate, a member of Detroit\\u2019s city council, in an interview for the New York Times.\", /*#__PURE__*/_jsxDEV(\"a\", {\n          className: \"referenceLink\",\n          href: \"#10\",\n          children: \"[10]\"\n        }, void 0, false, {\n          fileName: _jsxFileName,\n          lineNumber: 92,\n          columnNumber: 70\n        }, this), \" This past November, Tate voted to approve a contract extension for the facial recognition software that had misidentified Robert Williams earlier this year, arguing that some of the technology\\u2019s flaws can be overcome through regulatory policy. While Tate is aware that this solution is not ideal, he also wants to reframe the narrative on facial recognition: its success in solving cases should not overshadow the few incidents that occurred along the way. According to him, checks and balances have been implemented following William\\u2019s wrongful arrest. Those include requiring multiple approvals, limiting the use of facial recognition to serious crimes, and being supervised by a civilian oversight board.\"]\n      }, void 0, true, {\n        fileName: _jsxFileName,\n        lineNumber: 91,\n        columnNumber: 13\n      }, this), /*#__PURE__*/_jsxDEV(\"p\", {\n        children: \"The twitter API was instrumental in conducting this research. Racial bias in facial recognition technology is an issue that has gained a lot of traction over the past year and the number of tweets and resources on the topic are legion. The reason I selected this corpus of essays and articles is that they are very representative of the overarching debates on facial recognition that have taken place recently. Racial bias is not an isolated topic; as shown here, it is closely related to issues of privacy, ethics, and policy use. Each of the solutions proposed above to fight it offer their own advantages and drawbacks. IBM was one of the first to officially dismiss the technology as deeply flawed due to ethical and privacy concerns. Not only did the company deem it complicit in enacting new modes of racial profiling, but accessing diverse datasets proved challenging without violating privacy rights. Yet not everybody has ditched facial recognition. In an attempt to address these issues, a team of UK researchers is currently leveraging the power of GAN. Until significant improvements are made, regulating facial recognition is necessary. These diverse and sometimes polarized responses underscore the divisive, controversial nature of the technology and have compelled experts to reassess not just the conditions in which it should used, but whether it should be used at all given its limitations. Answering these questions is urgent. Until then, other Robert Williams will most likely suffer the consequences.\"\n      }, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 100,\n        columnNumber: 13\n      }, this), /*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 113,\n        columnNumber: 13\n      }, this), /*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 113,\n        columnNumber: 18\n      }, this), /*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 113,\n        columnNumber: 23\n      }, this), /*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 113,\n        columnNumber: 28\n      }, this), /*#__PURE__*/_jsxDEV(\"div\", {\n        className: \"references\",\n        children: [/*#__PURE__*/_jsxDEV(\"h3\", {\n          children: \"References\"\n        }, void 0, false, {\n          fileName: _jsxFileName,\n          lineNumber: 115,\n          columnNumber: 17\n        }, this), /*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n          fileName: _jsxFileName,\n          lineNumber: 116,\n          columnNumber: 17\n        }, this), /*#__PURE__*/_jsxDEV(\"div\", {\n          className: \"pad-left\",\n          children: [/*#__PURE__*/_jsxDEV(\"a\", {\n            className: \"referenceLink\",\n            href: \"https://www.detroitnews.com/story/news/local/detroit-city/2020/06/26/detroit-police-clear\\r -record-man-wrongfully-accused-facial-recognition-software/3259651001/\",\n            name: \"1\",\n            children: [\"[1] Rahal, S. and Hicks, M. \\u201CDetroit Police Work to Expunge Record of Man Wrongfully Accused with Facial Recognition.\\u201D \", /*#__PURE__*/_jsxDEV(\"span\", {\n              className: \"title1\",\n              children: \"The Detroit News\"\n            }, void 0, false, {\n              fileName: _jsxFileName,\n              lineNumber: 120,\n              columnNumber: 96\n            }, this), \", 26 June 2020.\"]\n          }, void 0, true, {\n            fileName: _jsxFileName,\n            lineNumber: 118,\n            columnNumber: 21\n          }, this), /*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n            fileName: _jsxFileName,\n            lineNumber: 122,\n            columnNumber: 21\n          }, this), /*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n            fileName: _jsxFileName,\n            lineNumber: 122,\n            columnNumber: 26\n          }, this), /*#__PURE__*/_jsxDEV(\"a\", {\n            className: \"referenceLink\",\n            href: \"https://doi.org/10.1108/JICES-05-2018-0050\",\n            name: \"2\",\n            children: [\"[2] Bacchini F. and Lorusso L. \\\"Race, again: how face recognition technology reinforces racial discrimination.\\\"\", /*#__PURE__*/_jsxDEV(\"span\", {\n              className: \"title1\",\n              children: \" Journal of Information, Communication and Ethics in Society\"\n            }, void 0, false, {\n              fileName: _jsxFileName,\n              lineNumber: 125,\n              columnNumber: 21\n            }, this), \", vol. 17, no. 3, 2019, pp. 321-335.\"]\n          }, void 0, true, {\n            fileName: _jsxFileName,\n            lineNumber: 123,\n            columnNumber: 21\n          }, this), /*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n            fileName: _jsxFileName,\n            lineNumber: 127,\n            columnNumber: 21\n          }, this), /*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n            fileName: _jsxFileName,\n            lineNumber: 127,\n            columnNumber: 26\n          }, this), /*#__PURE__*/_jsxDEV(\"a\", {\n            className: \"referenceLink\",\n            href: \"https://doi.org/10.1016/S0969-4765(19)30114-6\",\n            name: \"3\",\n            children: [\"[3] Bowyer, K. and King, M. \\u201CWhy face recognition accuracy varies due to race.\\u201D \", /*#__PURE__*/_jsxDEV(\"span\", {\n              className: \"title1\",\n              children: \"Biometric Technology Today\"\n            }, void 0, false, {\n              fileName: _jsxFileName,\n              lineNumber: 129,\n              columnNumber: 63\n            }, this), \", vol. 2019, no. 8, 2019, pp. 8-11.\"]\n          }, void 0, true, {\n            fileName: _jsxFileName,\n            lineNumber: 128,\n            columnNumber: 21\n          }, this), /*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n            fileName: _jsxFileName,\n            lineNumber: 131,\n            columnNumber: 21\n          }, this), /*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n            fileName: _jsxFileName,\n            lineNumber: 131,\n            columnNumber: 26\n          }, this), /*#__PURE__*/_jsxDEV(\"a\", {\n            className: \"referenceLink\",\n            href: \"https://journals.sagepub.com/doi/abs/10.1111/j.1467-9280.2007.02029.x\",\n            name: \"4\",\n            children: [\"[4] Kelly, D., Quinn, P., and Slater, A. \\u201CThe Other-Race Effect Develops During Infancy: Evidence of Perceptual Narrowing.\\u201D\", /*#__PURE__*/_jsxDEV(\"span\", {\n              className: \"title1\",\n              children: \" Psychological Science\"\n            }, void 0, false, {\n              fileName: _jsxFileName,\n              lineNumber: 135,\n              columnNumber: 25\n            }, this), \", vol. 18, no. 12, 2007.\"]\n          }, void 0, true, {\n            fileName: _jsxFileName,\n            lineNumber: 133,\n            columnNumber: 21\n          }, this), /*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n            fileName: _jsxFileName,\n            lineNumber: 137,\n            columnNumber: 21\n          }, this), /*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n            fileName: _jsxFileName,\n            lineNumber: 137,\n            columnNumber: 26\n          }, this), /*#__PURE__*/_jsxDEV(\"a\", {\n            className: \"referenceLink\",\n            href: \"https://doi.org/10.3758/BF03208892\",\n            name: \"5\",\n            children: [\"[5] O\\u2019toole, A.J., Deffenbacher, K.A., Valentin, D. et al. \\u201CStructural aspects of face recognition and the other-race effect.\\u201D \", /*#__PURE__*/_jsxDEV(\"span\", {\n              className: \"title1\",\n              children: \"Memory and Cognition\"\n            }, void 0, false, {\n              fileName: _jsxFileName,\n              lineNumber: 139,\n              columnNumber: 99\n            }, this), \", vol. 22, 1994, pp. 208\\u2013224.\"]\n          }, void 0, true, {\n            fileName: _jsxFileName,\n            lineNumber: 138,\n            columnNumber: 21\n          }, this), /*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n            fileName: _jsxFileName,\n            lineNumber: 142,\n            columnNumber: 21\n          }, this), /*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n            fileName: _jsxFileName,\n            lineNumber: 142,\n            columnNumber: 26\n          }, this), /*#__PURE__*/_jsxDEV(\"a\", {\n            className: \"referenceLink\",\n            href: \"http://gendershades.org/overview.html\",\n            name: \"6\",\n            children: [\"[6] Buolamwini, J., and Gebru, T.\\xA0\", /*#__PURE__*/_jsxDEV(\"span\", {\n              className: \"title1\",\n              children: \"Gender Shades\"\n            }, void 0, false, {\n              fileName: _jsxFileName,\n              lineNumber: 144,\n              columnNumber: 25\n            }, this), \".\"]\n          }, void 0, true, {\n            fileName: _jsxFileName,\n            lineNumber: 143,\n            columnNumber: 21\n          }, this), /*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n            fileName: _jsxFileName,\n            lineNumber: 146,\n            columnNumber: 21\n          }, this), /*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n            fileName: _jsxFileName,\n            lineNumber: 146,\n            columnNumber: 26\n          }, this), /*#__PURE__*/_jsxDEV(\"a\", {\n            className: \"referenceLink\",\n            href: \"https://www.ibm.com/blogs/research/2019/01/diversity-in-faces/\",\n            name: \"7\",\n            children: [\"[7] Smith, J. \\u201CIBM Research Releases 'Diversity in Faces' Dataset to Advance Study of Fairness in Facial Recognition Systems.\\u201D\", /*#__PURE__*/_jsxDEV(\"span\", {\n              className: \"title1\",\n              children: \" IBM Research Blog\"\n            }, void 0, false, {\n              fileName: _jsxFileName,\n              lineNumber: 149,\n              columnNumber: 25\n            }, this), \", 14 June 2020.\"]\n          }, void 0, true, {\n            fileName: _jsxFileName,\n            lineNumber: 147,\n            columnNumber: 21\n          }, this), /*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n            fileName: _jsxFileName,\n            lineNumber: 151,\n            columnNumber: 21\n          }, this), /*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n            fileName: _jsxFileName,\n            lineNumber: 151,\n            columnNumber: 26\n          }, this), /*#__PURE__*/_jsxDEV(\"a\", {\n            className: \"referenceLink\",\n            href: \"https://nbcnews.to/3qRN6WG\",\n            name: \"8\",\n            children: [\"[8] Solon, O. \\u201CFacial recognition's 'dirty little secret': Millions of online photos scraped without consent,\\u201D\", /*#__PURE__*/_jsxDEV(\"span\", {\n              className: \"title1\",\n              children: \" NBC News\"\n            }, void 0, false, {\n              fileName: _jsxFileName,\n              lineNumber: 154,\n              columnNumber: 25\n            }, this), \", March 12 2019.\"]\n          }, void 0, true, {\n            fileName: _jsxFileName,\n            lineNumber: 152,\n            columnNumber: 21\n          }, this), /*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n            fileName: _jsxFileName,\n            lineNumber: 156,\n            columnNumber: 21\n          }, this), /*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n            fileName: _jsxFileName,\n            lineNumber: 156,\n            columnNumber: 26\n          }, this), /*#__PURE__*/_jsxDEV(\"a\", {\n            className: \"referenceLink\",\n            href: \"https://arxiv.org/pdf/2004.08945v1.pdf\",\n            name: \"9\",\n            children: \"[9] Yucer, S., Ak\\xE7ay, S., et al. \\u201CExploring Racial Bias Within Face Recognition via Per-Subject Adversarially-Enabled Data Augmentation.\\u201D 19 April 2020.\"\n          }, void 0, false, {\n            fileName: _jsxFileName,\n            lineNumber: 157,\n            columnNumber: 21\n          }, this), /*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n            fileName: _jsxFileName,\n            lineNumber: 161,\n            columnNumber: 21\n          }, this), /*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n            fileName: _jsxFileName,\n            lineNumber: 161,\n            columnNumber: 26\n          }, this), /*#__PURE__*/_jsxDEV(\"a\", {\n            className: \"referenceLink\",\n            href: \"https://www.nytimes.com/2020/11/11/technology/facial-recognition-software-police.html\",\n            name: \"10\",\n            children: [\"[10] Ovide, S. \\u201CA Case for Facial Recognition.\\u201D \", /*#__PURE__*/_jsxDEV(\"span\", {\n              className: \"title1\",\n              children: \"The New York Times\"\n            }, void 0, false, {\n              fileName: _jsxFileName,\n              lineNumber: 163,\n              columnNumber: 79\n            }, this), \", 11 November 2020\"]\n          }, void 0, true, {\n            fileName: _jsxFileName,\n            lineNumber: 162,\n            columnNumber: 21\n          }, this)]\n        }, void 0, true, {\n          fileName: _jsxFileName,\n          lineNumber: 117,\n          columnNumber: 17\n        }, this), /*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n          fileName: _jsxFileName,\n          lineNumber: 166,\n          columnNumber: 17\n        }, this), /*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n          fileName: _jsxFileName,\n          lineNumber: 166,\n          columnNumber: 22\n        }, this), /*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n          fileName: _jsxFileName,\n          lineNumber: 166,\n          columnNumber: 27\n        }, this), /*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n          fileName: _jsxFileName,\n          lineNumber: 166,\n          columnNumber: 32\n        }, this)]\n      }, void 0, true, {\n        fileName: _jsxFileName,\n        lineNumber: 114,\n        columnNumber: 13\n      }, this)]\n    }, void 0, true, {\n      fileName: _jsxFileName,\n      lineNumber: 17,\n      columnNumber: 9\n    }, this)]\n  }, void 0, true, {\n    fileName: _jsxFileName,\n    lineNumber: 9,\n    columnNumber: 5\n  }, this)\n}, void 0, false);\n\n_c = HomePage;\nexport default HomePage;\n\nvar _c;\n\n$RefreshReg$(_c, \"HomePage\");","map":{"version":3,"sources":["E:/Software Development/SD744-FinalProject/my-blog/src/pages/HomePage.jsx"],"names":["React","img","figure1","figure2","figure3","HomePage"],"mappings":";;;AAAA,OAAOA,KAAP,MAAkB,OAAlB;AACA,OAAOC,GAAP,MAAgB,8BAAhB;AACA,OAAOC,OAAP,MAAoB,oBAApB;AACA,OAAOC,OAAP,MAAoB,oBAApB;AACA,OAAOC,OAAP,MAAoB,oBAApB;;AAEA,MAAMC,QAAQ,GAAG,mBACb;AAAA,yBACA;AAAK,IAAA,SAAS,EAAC,gBAAf;AAAA,4BACI;AAAK,MAAA,SAAS,EAAC,OAAf;AAAA,8BACI;AAAK,QAAA,SAAS,EAAC,QAAf;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,cADJ,eAEI;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,cAFJ;AAAA;AAAA;AAAA;AAAA;AAAA,YADJ,eAKI;AAAK,MAAA,SAAS,EAAC,OAAf;AAAA,6BACI;AAAK,QAAA,GAAG,EAAEJ,GAAV;AAAe,QAAA,GAAG,EAAC,gBAAnB;AAAoC,QAAA,KAAK,EAAC;AAA1C;AAAA;AAAA;AAAA;AAAA;AADJ;AAAA;AAAA;AAAA;AAAA,YALJ,eAQI;AAAK,MAAA,SAAS,EAAC,OAAf;AAAA,8BACI;AAAK,QAAA,SAAS,EAAC,aAAf;AAAA,gCACI;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,gBADJ,eAEI;AAAK,UAAA,SAAS,EAAC,MAAf;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,gBAFJ;AAAA;AAAA;AAAA;AAAA;AAAA,cADJ,eAKI;AAAA;AAAA;AAAA;AAAA,cALJ,eAMI;AAAA,8ZAG8C;AAAG,UAAA,SAAS,EAAC,eAAb;AAA6B,UAAA,IAAI,EAAC,IAAlC;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,gBAH9C;AAAA;AAAA;AAAA;AAAA;AAAA,cANJ,eAYI;AAAA,ujBAI4C;AAAG,UAAA,SAAS,EAAC,eAAb;AAA6B,UAAA,IAAI,EAAC,IAAlC;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,gBAJ5C,eAI0F;AAAG,UAAA,SAAS,EAAC,eAAb;AACtF,UAAA,IAAI,EAAC,IADiF;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,gBAJ1F;AAAA;AAAA;AAAA;AAAA;AAAA,cAZJ,eAsBI;AAAA,4SAEI;AAAG,UAAA,SAAS,EAAC,eAAb;AAA6B,UAAA,IAAI,EAAC,IAAlC;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,gBAFJ,eAEkD;AAAG,UAAA,SAAS,EAAC,eAAb;AAA6B,UAAA,IAAI,EAAC,IAAlC;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,gBAFlD,ghBAMgG;AAAG,UAAA,SAAS,EAAC,eAAb;AAC5F,UAAA,IAAI,EAAC,IADuF;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,gBANhG;AAAA;AAAA;AAAA;AAAA;AAAA,cAtBJ,eA+BI;AAAA,gCACI;AAAA;AAAA;AAAA;AAAA,gBADJ,eACS;AAAA;AAAA;AAAA;AAAA,gBADT,eAEI;AAAK,UAAA,GAAG,EAAEG,OAAV;AAAmB,UAAA,GAAG,EAAC,gBAAvB;AAAwC,UAAA,SAAS,EAAC;AAAlD;AAAA;AAAA;AAAA;AAAA,gBAFJ,eAGI;AAAA;AAAA;AAAA;AAAA,gBAHJ,eAGS;AAAA;AAAA;AAAA;AAAA,gBAHT;AAAA;AAAA;AAAA;AAAA;AAAA,cA/BJ,eAoCI;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,cApCJ,eA2CI;AAAA,ydAG+D;AAAG,UAAA,SAAS,EAAC,eAAb;AAA6B,UAAA,IAAI,EAAC,IAAlC;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,gBAH/D,8dAOuF;AAAG,UAAA,SAAS,EAAC,eAAb;AACnF,UAAA,IAAI,EAAC,IAD8E;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,gBAPvF;AAAA;AAAA;AAAA;AAAA;AAAA,cA3CJ,eAuDI;AAAA,gCACI;AAAA;AAAA;AAAA;AAAA,gBADJ,eACS;AAAA;AAAA;AAAA;AAAA,gBADT,eAEI;AAAK,UAAA,GAAG,EAAEF,OAAV;AAAmB,UAAA,GAAG,EAAC,gBAAvB;AAAwC,UAAA,SAAS,EAAC;AAAlD;AAAA;AAAA;AAAA;AAAA,gBAFJ,eAGI;AAAA;AAAA;AAAA;AAAA,gBAHJ,eAGS;AAAA;AAAA;AAAA;AAAA,gBAHT;AAAA;AAAA;AAAA;AAAA;AAAA,cAvDJ,eA4DI;AAAA,uVAEgE;AAAG,UAAA,SAAS,EAAC,eAAb;AAA6B,UAAA,IAAI,EAAC,IAAlC;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,gBAFhE;AAAA;AAAA;AAAA;AAAA;AAAA,cA5DJ,eAqEI;AAAA,gCACI;AAAA;AAAA;AAAA;AAAA,gBADJ,eACS;AAAA;AAAA;AAAA;AAAA,gBADT,eAEI;AAAK,UAAA,GAAG,EAAEC,OAAV;AAAmB,UAAA,GAAG,EAAC,gBAAvB;AAAwC,UAAA,KAAK,EAAC;AAA9C;AAAA;AAAA;AAAA;AAAA,gBAFJ,eAGI;AAAA;AAAA;AAAA;AAAA,gBAHJ,eAGS;AAAA;AAAA;AAAA;AAAA,gBAHT;AAAA;AAAA;AAAA;AAAA;AAAA,cArEJ,eA0EI;AAAA,8NACyD;AAAG,UAAA,SAAS,EAAC,eAAb;AAA6B,UAAA,IAAI,EAAC,KAAlC;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,gBADzD;AAAA;AAAA;AAAA;AAAA;AAAA,cA1EJ,eAmFI;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,cAnFJ,eAgGI;AAAA;AAAA;AAAA;AAAA,cAhGJ,eAgGS;AAAA;AAAA;AAAA;AAAA,cAhGT,eAgGc;AAAA;AAAA;AAAA;AAAA,cAhGd,eAgGmB;AAAA;AAAA;AAAA;AAAA,cAhGnB,eAiGI;AAAK,QAAA,SAAS,EAAC,YAAf;AAAA,gCACI;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,gBADJ,eAEI;AAAA;AAAA;AAAA;AAAA,gBAFJ,eAGI;AAAK,UAAA,SAAS,EAAC,UAAf;AAAA,kCACI;AAAG,YAAA,SAAS,EAAC,eAAb;AAA6B,YAAA,IAAI,EAAC,oKAAlC;AACwE,YAAA,IAAI,EAAC,GAD7E;AAAA,yKAE2E;AAAM,cAAA,SAAS,EAAC,QAAhB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,oBAF3E;AAAA;AAAA;AAAA;AAAA;AAAA,kBADJ,eAKI;AAAA;AAAA;AAAA;AAAA,kBALJ,eAKS;AAAA;AAAA;AAAA;AAAA,kBALT,eAMI;AAAG,YAAA,SAAS,EAAC,eAAb;AAA6B,YAAA,IAAI,EAAC,4CAAlC;AAA+E,YAAA,IAAI,EAAC,GAApF;AAAA,yJAEA;AAAM,cAAA,SAAS,EAAC,QAAhB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,oBAFA;AAAA;AAAA;AAAA;AAAA;AAAA,kBANJ,eAUI;AAAA;AAAA;AAAA;AAAA,kBAVJ,eAUS;AAAA;AAAA;AAAA;AAAA,kBAVT,eAWI;AAAG,YAAA,SAAS,EAAC,eAAb;AAA6B,YAAA,IAAI,EAAC,+CAAlC;AAAkF,YAAA,IAAI,EAAC,GAAvF;AAAA,kIAC0C;AAAM,cAAA,SAAS,EAAC,QAAhB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,oBAD1C;AAAA;AAAA;AAAA;AAAA;AAAA,kBAXJ,eAcI;AAAA;AAAA;AAAA;AAAA,kBAdJ,eAcS;AAAA;AAAA;AAAA;AAAA,kBAdT,eAgBI;AAAG,YAAA,SAAS,EAAC,eAAb;AAA6B,YAAA,IAAI,EAAC,uEAAlC;AAA0G,YAAA,IAAI,EAAC,GAA/G;AAAA,6KAEI;AAAM,cAAA,SAAS,EAAC,QAAhB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,oBAFJ;AAAA;AAAA;AAAA;AAAA;AAAA,kBAhBJ,eAoBI;AAAA;AAAA;AAAA;AAAA,kBApBJ,eAoBS;AAAA;AAAA;AAAA;AAAA,kBApBT,eAqBI;AAAG,YAAA,SAAS,EAAC,eAAb;AAA6B,YAAA,IAAI,EAAC,oCAAlC;AAAuE,YAAA,IAAI,EAAC,GAA5E;AAAA,sLAC8E;AAAM,cAAA,SAAS,EAAC,QAAhB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,oBAD9E;AAAA;AAAA;AAAA;AAAA;AAAA,kBArBJ,eAyBI;AAAA;AAAA;AAAA;AAAA,kBAzBJ,eAyBS;AAAA;AAAA;AAAA;AAAA,kBAzBT,eA0BI;AAAG,YAAA,SAAS,EAAC,eAAb;AAA6B,YAAA,IAAI,EAAC,uCAAlC;AAA0E,YAAA,IAAI,EAAC,GAA/E;AAAA,6EACI;AAAM,cAAA,SAAS,EAAC,QAAhB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,oBADJ;AAAA;AAAA;AAAA;AAAA;AAAA,kBA1BJ,eA6BI;AAAA;AAAA;AAAA;AAAA,kBA7BJ,eA6BS;AAAA;AAAA;AAAA;AAAA,kBA7BT,eA8BI;AAAG,YAAA,SAAS,EAAC,eAAb;AAA6B,YAAA,IAAI,EAAC,gEAAlC;AAAmG,YAAA,IAAI,EAAC,GAAxG;AAAA,gLAEI;AAAM,cAAA,SAAS,EAAC,QAAhB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,oBAFJ;AAAA;AAAA;AAAA;AAAA;AAAA,kBA9BJ,eAkCI;AAAA;AAAA;AAAA;AAAA,kBAlCJ,eAkCS;AAAA;AAAA;AAAA;AAAA,kBAlCT,eAmCI;AAAG,YAAA,SAAS,EAAC,eAAb;AAA6B,YAAA,IAAI,EAAC,4BAAlC;AAA+D,YAAA,IAAI,EAAC,GAApE;AAAA,gKAEI;AAAM,cAAA,SAAS,EAAC,QAAhB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,oBAFJ;AAAA;AAAA;AAAA;AAAA;AAAA,kBAnCJ,eAuCI;AAAA;AAAA;AAAA;AAAA,kBAvCJ,eAuCS;AAAA;AAAA;AAAA;AAAA,kBAvCT,eAwCI;AAAG,YAAA,SAAS,EAAC,eAAb;AAA6B,YAAA,IAAI,EAAC,wCAAlC;AAA2E,YAAA,IAAI,EAAC,GAAhF;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,kBAxCJ,eA4CI;AAAA;AAAA;AAAA;AAAA,kBA5CJ,eA4CS;AAAA;AAAA;AAAA;AAAA,kBA5CT,eA6CI;AAAG,YAAA,SAAS,EAAC,eAAb;AAA6B,YAAA,IAAI,EAAC,uFAAlC;AACA,YAAA,IAAI,EAAC,IADL;AAAA,kGAC0D;AAAM,cAAA,SAAS,EAAC,QAAhB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,oBAD1D;AAAA;AAAA;AAAA;AAAA;AAAA,kBA7CJ;AAAA;AAAA;AAAA;AAAA;AAAA,gBAHJ,eAoDI;AAAA;AAAA;AAAA;AAAA,gBApDJ,eAoDS;AAAA;AAAA;AAAA;AAAA,gBApDT,eAoDc;AAAA;AAAA;AAAA;AAAA,gBApDd,eAoDmB;AAAA;AAAA;AAAA;AAAA,gBApDnB;AAAA;AAAA;AAAA;AAAA;AAAA,cAjGJ;AAAA;AAAA;AAAA;AAAA;AAAA,YARJ;AAAA;AAAA;AAAA;AAAA;AAAA;AADA,iBADJ;;KAAME,Q;AAsKN,eAAeA,QAAf","sourcesContent":["import React from 'react';\r\nimport img from '../img/facialrecognition.jpg';\r\nimport figure1 from '../img/Figure1.jpg';\r\nimport figure2 from '../img/Figure2.jpg';\r\nimport figure3 from '../img/Figure3.jpg';\r\n\r\nconst HomePage = () => (\r\n    <>\r\n    <div className=\"grid-container\">\r\n        <div className=\"cell1\">\r\n            <div className=\"title1\">\"I guess the computer got it wrong\"</div>\r\n            <div>Uncovering Racial Bias in Facial Recognition Technology</div>\r\n        </div>\r\n        <div className=\"cell2\">\r\n            <img src={img} alt=\"cannot display\" width=\"100%\"/>  \r\n        </div>\r\n        <div className=\"cell3\">\r\n            <div className=\"border-bttm\">\r\n                <h3>Romain Delaville</h3>\r\n                <div className=\"date\">DEC 15, 2020</div>\r\n            </div>\r\n            <br/>\r\n            <p>Earlier this year, Robert Williams, 42, was arrested on suspicion of stealing watches from a trendy Shinola \r\n                retail store he had not patronized in years. Falsely identified as the suspect by Michigan State Police's \r\n                face recognition software, he spent a total of 30 hours in detention. Charges were dismissed once investigators \r\n                realized that “the computer got it wrong,”<a className=\"referenceLink\" href=\"#1\">[1]</a> making this incident \r\n                the first wrongful arrest in the U.S. due to a facial recognition mismatch.\r\n            </p>\r\n            <p>Since its early days, facial recognition has raised questions among privacy advocates and ethics watchdogs who have \r\n                repeatedly warned against data collection, processing, and storage methods as well as unregulated use of the technology. \r\n                William’s story brought to light yet another – and perhaps more radical – set of concerns: just how reliable is facial \r\n                recognition to begin with? Can it be trusted at all? Last year, some scholars had already exposed what they describe as \r\n                racial bias in facial biometric systems.<a className=\"referenceLink\" href=\"#2\">[2]</a><a className=\"referenceLink\" \r\n                href=\"#3\">[3]</a> In what follows, I will examine this issue in the context of its use by law enforcement. I will draw \r\n                on a corpus of tweets, newspaper articles, and scholarly essays made available by the Twitter API to unpack this notion \r\n                of racial bias, identify its origins, and discuss recent efforts to correct it. Throughout this essay, I will be using the \r\n                term “race” instead of “ethnicity” to align with the terminology adopted in academic studies. \r\n            </p>\r\n            <p>Social psychologists agree that humans tend to perceive faces of their own race more acutely than those of other races. \r\n                Referred to as “cross-race effect” or “other-race effect,” this phenomenon has been supported by a large corpus of evidence. \r\n                <a className=\"referenceLink\" href=\"#4\">[4]</a><a className=\"referenceLink\" href=\"#5\">[5]</a> Some hypotheses suggest that the \r\n                amount of exposure to and experience with a specific race group is correlated with the ability to recognize individuals from \r\n                that group. This could explain how some systems struggle to identify people of color, especially women. According to the 2018 \r\n                “Gender Shades” project, algorithms developed by Microsoft, Amazon, and IBM, among others, yielded disconcerting results.  \r\n                Darker-skinned women were over 34% more likely to be misidentified than light-skinned males.<a className=\"referenceLink\" \r\n                href=\"#6\">[6]</a> This begs the question: how can something as inherently human as perceptual bias be embedded in code? \r\n            </p>\r\n            <p>\r\n                <br/><br/>\r\n                <img src={figure3} alt=\"cannot display\" className=\"figure3\"/>\r\n                <br/><br/>\r\n            </p>\r\n            <p>Nowadays, facial recognition systems rely on machine-learning frameworks to train complex algorithms by feeding them large \r\n                image datasets. Those images are generally categorized by age, gender, skin tone, and a variety of other metrics. The \r\n                more an algorithm is fed images of diverse faces, the more proficient it becomes in identifying those faces. One possible \r\n                explanation for the racial bias of facial recognition systems may thus have to do with low public availability of diverse \r\n                datasets – those are indeed quite rare. A consequence of this shortage is to encode the other-race effect directly into \r\n                the software application layer as algorithms struggle to recognize faces that they have been less exposed to.\r\n            </p>\r\n            <p>If performance bias does stem from a lack of representation in data, then what are companies and organizations doing to \r\n                address it? In January, IBM decided to publicly release a database of a million faces that better reflect the variety \r\n                of skin tones found in the real world. Named “Diversity in Faces,” this massive dataset was meant “to advance the study \r\n                of fairness and accuracy in facial recognition technology.”<a className=\"referenceLink\" href=\"#7\">[7]</a> However, IBM’s \r\n                initiative quickly backfired when NBC News journalist Olivia Solon revealed that many of those images were collected \r\n                from the photo-hosting site Flickr without consent. While IBM guaranteed users that they could opt out of the database, \r\n                doing so proved very difficult. Indeed, IBM required them to email links to the images they wanted removed, but never \r\n                publicly shared the list of Flickr users whose photos were included in the dataset.<a className=\"referenceLink\" \r\n                href=\"#8\">[8]</a> In the wake of this controversy and as Black Lives Matter protests erupted all over the country \r\n                following George Floyd’s death, IBM ultimately decided to abandon its facial recognition research program, implying \r\n                that the only way to get rid of facial recognition bias was to get rid of facial recognition altogether.\r\n            </p>\r\n            <p>\r\n                <br/><br/>\r\n                <img src={figure1} alt=\"cannot display\" className=\"figure1\"/>\r\n                <br/><br/>\r\n            </p>\r\n            <p>But not everyone gave up on improving the technology. In March of this year, a group of computer scientists and engineers \r\n                at the University of Durham, England, proposed an innovative method to reduce bias occurrences without infringing on privacy \r\n                rights: per-subject adversarially-enabled data augmentation.<a className=\"referenceLink\" href=\"#9\">[9]</a> This approach \r\n                uses generative adversarial network (GAN) to transfer racial attributes of a human face while preserving individual identity \r\n                features. This makes it possible to produce different versions of the same face across multiple race groups, thus increasing \r\n                the diversity of dataset images. In other words, adversarially-enabled data augmentation attempt to mitigate bias at the \r\n                pre-processing level (data preparation) by creating fair datasets. While accuracy gains were made, they seemed minimal and \r\n                only went up by 1%. Could it be that GAN, a fairly new technology that was developed in 2014, may itself need further correction?\r\n            </p>\r\n            <p>\r\n                <br/><br/>\r\n                <img src={figure2} alt=\"cannot display\" width=\"100%\" />\r\n                <br/><br/>\r\n            </p>\r\n            <p>A third way to solve some of the issues that have plagued facial recognition was suggested by James Tate, a member of Detroit’s \r\n                city council, in an interview for the New York Times.<a className=\"referenceLink\" href=\"#10\">[10]</a> This past November, Tate \r\n                voted to approve a contract extension for the facial recognition software that had misidentified Robert Williams earlier this \r\n                year, arguing that some of the technology’s flaws can be overcome through regulatory policy. While Tate is aware that this \r\n                solution is not ideal, he also wants to reframe the narrative on facial recognition: its success in solving cases should not \r\n                overshadow the few incidents that occurred along the way. According to him, checks and balances have been implemented following \r\n                William’s wrongful arrest. Those include requiring multiple approvals, limiting the use of facial recognition to serious crimes, \r\n                and being supervised by a civilian oversight board.\r\n            </p>\r\n            <p>The twitter API was instrumental in conducting this research. Racial bias in facial recognition technology is an issue that has \r\n                gained a lot of traction over the past year and the number of tweets and resources on the topic are legion. The reason I \r\n                selected this corpus of essays and articles is that they are very representative of the overarching debates on facial recognition \r\n                that have taken place recently. Racial bias is not an isolated topic; as shown here, it is closely related to issues of privacy, \r\n                ethics, and policy use. Each of the solutions proposed above to fight it offer their own advantages and drawbacks. IBM was one of \r\n                the first to officially dismiss the technology as deeply flawed due to ethical and privacy concerns. Not only did the company \r\n                deem it complicit in enacting new modes of racial profiling, but accessing diverse datasets proved challenging without violating \r\n                privacy rights. Yet not everybody has ditched facial recognition. In an attempt to address these issues, a team of UK researchers \r\n                is currently leveraging the power of GAN. Until significant improvements are made, regulating facial recognition is necessary. \r\n                These diverse and sometimes polarized responses underscore the divisive, controversial nature of the technology and have compelled \r\n                experts to reassess not just the conditions in which it should used, but whether it should be used at all given its limitations. \r\n                Answering these questions is urgent. Until then, other Robert Williams will most likely suffer the consequences. \r\n            </p>\r\n            <br/><br/><br/><br/>\r\n            <div className=\"references\">\r\n                <h3>References</h3>\r\n                <br/>\r\n                <div className=\"pad-left\">\r\n                    <a className=\"referenceLink\" href=\"https://www.detroitnews.com/story/news/local/detroit-city/2020/06/26/detroit-police-clear\r\n                    -record-man-wrongfully-accused-facial-recognition-software/3259651001/\" name=\"1\">[1] Rahal, S. and Hicks, M. “Detroit Police \r\n                    Work to Expunge Record of Man Wrongfully Accused with Facial Recognition.” <span className=\"title1\">The Detroit News</span>, 26 June 2020.\r\n                    </a>\r\n                    <br/><br/> \r\n                    <a className=\"referenceLink\" href=\"https://doi.org/10.1108/JICES-05-2018-0050\" name=\"2\">[2] Bacchini F. and Lorusso L. \"Race, again: \r\n                    how face recognition technology reinforces racial discrimination.\" \r\n                    <span className=\"title1\"> Journal of Information, Communication and Ethics in Society</span>, vol. 17, no. 3, 2019, pp. 321-335. \r\n                    </a>\r\n                    <br/><br/>\r\n                    <a className=\"referenceLink\" href=\"https://doi.org/10.1016/S0969-4765(19)30114-6\" name=\"3\">[3] Bowyer, K. and King, M. “Why face \r\n                    recognition accuracy varies due to race.” <span className=\"title1\">Biometric Technology Today</span>, vol. 2019, no. 8, 2019, pp. 8-11. \r\n                    </a>\r\n                    <br/><br/>\r\n\r\n                    <a className=\"referenceLink\" href=\"https://journals.sagepub.com/doi/abs/10.1111/j.1467-9280.2007.02029.x\" name=\"4\">\r\n                        [4] Kelly, D., Quinn, P., and Slater, A. “The Other-Race Effect Develops During Infancy: Evidence of Perceptual Narrowing.” \r\n                        <span className=\"title1\"> Psychological Science</span>, vol. 18, no. 12, 2007. \r\n                    </a>\r\n                    <br/><br/>\r\n                    <a className=\"referenceLink\" href=\"https://doi.org/10.3758/BF03208892\" name=\"5\">[5] O’toole, A.J., Deffenbacher, K.A., Valentin, \r\n                    D. et al. “Structural aspects of face recognition and the other-race effect.” <span className=\"title1\">Memory and Cognition</span>, \r\n                    vol. 22, 1994, pp. 208–224. \r\n                    </a>\r\n                    <br/><br/>\r\n                    <a className=\"referenceLink\" href=\"http://gendershades.org/overview.html\" name=\"6\">[6] Buolamwini, J., and Gebru, T.&nbsp; \r\n                        <span className=\"title1\">Gender Shades</span>.\r\n                    </a>\r\n                    <br/><br/>\r\n                    <a className=\"referenceLink\" href=\"https://www.ibm.com/blogs/research/2019/01/diversity-in-faces/\" name=\"7\">\r\n                        [7] Smith, J. “IBM Research Releases 'Diversity in Faces' Dataset to Advance Study of Fairness in Facial Recognition Systems.” \r\n                        <span className=\"title1\"> IBM Research Blog</span>, 14 June 2020.\r\n                    </a>\r\n                    <br/><br/>\r\n                    <a className=\"referenceLink\" href=\"https://nbcnews.to/3qRN6WG\" name=\"8\">\r\n                        [8] Solon, O. “Facial recognition's 'dirty little secret': Millions of online photos scraped without consent,” \r\n                        <span className=\"title1\"> NBC News</span>, March 12 2019.\r\n                    </a>\r\n                    <br/><br/>\r\n                    <a className=\"referenceLink\" href=\"https://arxiv.org/pdf/2004.08945v1.pdf\" name=\"9\">\r\n                        [9] Yucer, S., Akçay, S., et al. “Exploring Racial Bias Within Face Recognition via Per-Subject Adversarially-Enabled Data \r\n                        Augmentation.” 19 April 2020.\r\n                    </a>\r\n                    <br/><br/>\r\n                    <a className=\"referenceLink\" href=\"https://www.nytimes.com/2020/11/11/technology/facial-recognition-software-police.html\" \r\n                    name=\"10\">[10] Ovide, S. “A Case for Facial Recognition.” <span className=\"title1\">The New York Times</span>, 11 November 2020\r\n                    </a>\r\n                </div>\r\n                <br/><br/><br/><br/>\r\n            </div>\r\n        </div>  \r\n    </div>   \r\n    </> \r\n);\r\n\r\nexport default HomePage;\r\n"]},"metadata":{},"sourceType":"module"}