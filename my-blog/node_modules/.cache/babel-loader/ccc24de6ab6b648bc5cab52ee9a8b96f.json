{"ast":null,"code":"import { jsxDEV as _jsxDEV } from \"react/jsx-dev-runtime\";\nimport { Fragment as _Fragment } from \"react/jsx-dev-runtime\";\nvar _jsxFileName = \"E:\\\\Software Development\\\\React\\\\my-blog\\\\src\\\\pages\\\\HomePage.jsx\";\nimport React from 'react';\nimport img from '../img/facial2.jpg';\n\nconst HomePage = () => /*#__PURE__*/_jsxDEV(_Fragment, {\n  children: /*#__PURE__*/_jsxDEV(\"div\", {\n    className: \"grid-container\",\n    children: [/*#__PURE__*/_jsxDEV(\"div\", {\n      className: \"cell1\",\n      children: /*#__PURE__*/_jsxDEV(\"div\", {\n        children: [\"\\\"I guess the computer got it wrong\\\"\", /*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n          fileName: _jsxFileName,\n          lineNumber: 8,\n          columnNumber: 49\n        }, this), \"Uncovering Racial Bias in Facial Recognition Technology\"]\n      }, void 0, true, {\n        fileName: _jsxFileName,\n        lineNumber: 8,\n        columnNumber: 9\n      }, this)\n    }, void 0, false, {\n      fileName: _jsxFileName,\n      lineNumber: 7,\n      columnNumber: 5\n    }, this), /*#__PURE__*/_jsxDEV(\"div\", {\n      className: \"cell2\",\n      children: /*#__PURE__*/_jsxDEV(\"img\", {\n        src: img,\n        alt: \"cannot display\",\n        width: \"100%\"\n      }, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 11,\n        columnNumber: 9\n      }, this)\n    }, void 0, false, {\n      fileName: _jsxFileName,\n      lineNumber: 10,\n      columnNumber: 5\n    }, this), /*#__PURE__*/_jsxDEV(\"div\", {\n      className: \"cell3\",\n      children: [/*#__PURE__*/_jsxDEV(\"p\", {\n        children: \"Earlier this year, Robert Williams, 42, was arrested on suspicion of stealing watches from a trendy Shinola retail store he had not patronized in years. Falsely identified as the suspect by Michigan State Police's face recognition software, he spent a total of 30 hours in detention. Charges were dismissed once investigators realized that \\u201Cthe computer got it wrong,\\u201D making this incident the first wrongful arrest in the U.S. due to a facial recognition mismatch.\"\n      }, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 14,\n        columnNumber: 9\n      }, this), /*#__PURE__*/_jsxDEV(\"p\", {\n        children: \"Since its early days, facial recognition has raised questions among privacy advocates and ethics watchdogs who worry about data collection, processing, and storage methods as well as unregulated use of the technology. William\\u2019s story brought to public attention yet another\\u2013 and perhaps more radical \\u2013 set of concerns: just how reliable is facial recognition? Can it be trusted at all? Long before his arrest, some scholars had already warned against varying error rates across demographic groups, which they viewed as a clear example of racial bias in facial biometric systems.  In what follows, I will draw on corpus of tweets, newspaper articles, and scholarly essays to unpack this notion, identify its origins, and discuss recent efforts to correct it. I will be using the term \\u201Crace\\u201D instead of \\u201Cethnicity\\u201D to align with the terminology adopted in academic studies.\"\n      }, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 20,\n        columnNumber: 9\n      }, this), /*#__PURE__*/_jsxDEV(\"p\", {\n        children: \"Social psychologists agree that humans tend to perceive faces of their own race more acutely than those of other races. Referred to as \\u201Ccross-race effect\\u201D or \\u201Cother-race effect,\\u201D this phenomenon has been supported by a large corpus of evidence.  Most hypotheses suggest that the amount of exposure to and experience with a specific race group is correlated with the ability to recognize individuals from that group. This other-race effect may very well explain how some systems struggle to identify people of color, especially women. According to the 2018 \\u201CGender Shades\\u201D project, algorithms developed by Microsoft, Amazon, and IBM yielded disconcerting results.  Darker-skinned women were over 34% more likely to be misidentified than light-skinned males. This begs the question: how can something as inherently human as perceptual bias be embedded in code?\"\n      }, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 31,\n        columnNumber: 9\n      }, this), /*#__PURE__*/_jsxDEV(\"p\", {\n        children: \"Nowadays, facial recognition systems rely on machine-learning frameworks to train complex algorithms by feeding them large image datasets. Those images are generally categorized by age, gender, skin tone, and a variety of other metrics. The more an algorithm is fed images of diverse faces, the more proficient it becomes in identifying those faces. One possible explanation for the racial bias of facial recognition systems may thus have to do with low public availability of diverse datasets \\u2013 those are indeed quite rare. The consequence is to encode the \\u201Cother-race effect\\u201D into the software application layer in such a way that algorithms ultimately become more apt to recognize faces that they have been exposed to/fed the most.\"\n      }, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 41,\n        columnNumber: 9\n      }, this), /*#__PURE__*/_jsxDEV(\"p\", {\n        children: \"If indeed performance bias stems from a lack of representation in data, then what are companies and organizations doing to address it? In January, IBM decided to publicly release a database of a million faces that better reflect the variety of skin tones found in the real world. Named \\u201CDiversity in Faces,\\u201D this massive dataset was meant \\u201Cto advance the study of fairness and accuracy in facial recognition technology.\\u201D  However, IBM\\u2019s initiative quickly backfired when NBC News journalist Olivia Solon revealed that many of those images of ordinary people were collected from the photo-hosting site Flickr without their consent. While IBM guaranteed users that they could opt out of the database, doing so proved very difficult. Indeed, IBM required them to email links to the images they wanted removed, but never publicly shared the list of Flickr users whose photos were included in the dataset. A year later, as Black Lives Matter protests erupted all over the country following George Floyd\\u2019s death, IBM ultimately decided to abandon its facial recognition research program, possibly implying that the only way to get rid of facial recognition bias was to get rid of facial recognition altogether.\"\n      }, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 50,\n        columnNumber: 9\n      }, this), /*#__PURE__*/_jsxDEV(\"p\", {\n        children: \"But not everyone gave up on improving the technology. In March of this year, a group of computer scientists and engineers at the University of Durham proposed an innovative method to reduce bias occurrences without infringing on privacy rights: per-subject adversarially-enabled data augmentation. What this does is leverage the power of generative adversarial network (GAN) to transfer racial attributes of a human face while preserving identity features. This technique allows to produce different versions of the same face across multiple race groups, thus increasing the diversity of dataset images. However, some limitations must be noted. First, while accuracy gains were made, they seemed minimal and only went up by 1%. GAN is a fairly new technology developed in 2014 and may itself need further correction? Second, the race groups that this technology was tested on are few: Caucasian, African, Indian, and Asian. A pressing question, then, is how to accommodate individuals who do not fall within any of these groups (e.g. Pacific islanders) or whose traits are distributed across them.\"\n      }, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 64,\n        columnNumber: 9\n      }, this), /*#__PURE__*/_jsxDEV(\"p\", {\n        children: \"Each of the solutions discussed above \\u2013 IBM\\u2019s disavowal of facial recognition and adversarially-enabled data augmentation \\u2013 their own merits and flaws.\"\n      }, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 76,\n        columnNumber: 9\n      }, this), /*#__PURE__*/_jsxDEV(\"p\", {\n        children: \" Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\"\n      }, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 79,\n        columnNumber: 9\n      }, this), /*#__PURE__*/_jsxDEV(\"p\", {\n        children: \" Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\"\n      }, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 86,\n        columnNumber: 9\n      }, this)]\n    }, void 0, true, {\n      fileName: _jsxFileName,\n      lineNumber: 13,\n      columnNumber: 5\n    }, this)]\n  }, void 0, true, {\n    fileName: _jsxFileName,\n    lineNumber: 6,\n    columnNumber: 1\n  }, this)\n}, void 0, false);\n\n_c = HomePage;\nexport default HomePage;\n\nvar _c;\n\n$RefreshReg$(_c, \"HomePage\");","map":{"version":3,"sources":["E:/Software Development/React/my-blog/src/pages/HomePage.jsx"],"names":["React","img","HomePage"],"mappings":";;;AAAA,OAAOA,KAAP,MAAkB,OAAlB;AACA,OAAOC,GAAP,MAAgB,oBAAhB;;AAEA,MAAMC,QAAQ,GAAG,mBACjB;AAAA,yBACA;AAAK,IAAA,SAAS,EAAC,gBAAf;AAAA,4BACI;AAAK,MAAA,SAAS,EAAC,OAAf;AAAA,6BACI;AAAA,yEAAwC;AAAA;AAAA;AAAA;AAAA,gBAAxC;AAAA;AAAA;AAAA;AAAA;AAAA;AADJ;AAAA;AAAA;AAAA;AAAA,YADJ,eAII;AAAK,MAAA,SAAS,EAAC,OAAf;AAAA,6BACI;AAAK,QAAA,GAAG,EAAED,GAAV;AAAe,QAAA,GAAG,EAAC,gBAAnB;AAAoC,QAAA,KAAK,EAAC;AAA1C;AAAA;AAAA;AAAA;AAAA;AADJ;AAAA;AAAA;AAAA;AAAA,YAJJ,eAOI;AAAK,MAAA,SAAS,EAAC,OAAf;AAAA,8BACI;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,cADJ,eAOI;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,cAPJ,eAkBI;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,cAlBJ,eA4BI;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,cA5BJ,eAqCI;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,cArCJ,eAmDI;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,cAnDJ,eA+DI;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,cA/DJ,eAkEI;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,cAlEJ,eAyEI;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,cAzEJ;AAAA;AAAA;AAAA;AAAA;AAAA,YAPJ;AAAA;AAAA;AAAA;AAAA;AAAA;AADA,iBADA;;KAAMC,Q;AA+FN,eAAeA,QAAf","sourcesContent":["import React from 'react';\r\nimport img from '../img/facial2.jpg';\r\n\r\nconst HomePage = () => ( \r\n<>\r\n<div className=\"grid-container\">\r\n    <div className=\"cell1\">\r\n        <div>\"I guess the computer got it wrong\"<br/>Uncovering Racial Bias in Facial Recognition Technology</div>\r\n    </div>\r\n    <div className=\"cell2\">\r\n        <img src={img} alt=\"cannot display\" width=\"100%\"/>  \r\n    </div>\r\n    <div className=\"cell3\">\r\n        <p>Earlier this year, Robert Williams, 42, was arrested on suspicion of stealing watches from a trendy \r\n            Shinola retail store he had not patronized in years. Falsely identified as the suspect by Michigan \r\n            State Police's face recognition software, he spent a total of 30 hours in detention. Charges were \r\n            dismissed once investigators realized that “the computer got it wrong,” making this incident the first \r\n            wrongful arrest in the U.S. due to a facial recognition mismatch.\r\n        </p>\r\n        <p>\r\n        Since its early days, facial recognition has raised questions among privacy advocates and ethics watchdogs \r\n        who worry about data collection, processing, and storage methods as well as unregulated use of the \r\n        technology. William’s story brought to public attention yet another– and perhaps more radical – set of \r\n        concerns: just how reliable is facial recognition? Can it be trusted at all? Long before his arrest, some \r\n        scholars had already warned against varying error rates across demographic groups, which they viewed as a \r\n        clear example of racial bias in facial biometric systems.  In what follows, I will draw on corpus of \r\n        tweets, newspaper articles, and scholarly essays to unpack this notion, identify its origins, and discuss \r\n        recent efforts to correct it. I will be using the term “race” instead of “ethnicity” to align with the \r\n        terminology adopted in academic studies. \r\n        </p>\r\n        <p>Social psychologists agree that humans tend to perceive faces of their own race more acutely than those \r\n            of other races. Referred to as “cross-race effect” or “other-race effect,” this phenomenon has been \r\n            supported by a large corpus of evidence.  Most hypotheses suggest that the amount of exposure to and \r\n            experience with a specific race group is correlated with the ability to recognize individuals from that \r\n            group. This other-race effect may very well explain how some systems struggle to identify people of \r\n            color, especially women. According to the 2018 “Gender Shades” project, algorithms developed by \r\n            Microsoft, Amazon, and IBM yielded disconcerting results.  Darker-skinned women were over 34% more \r\n            likely to be misidentified than light-skinned males. This begs the question: how can something as \r\n            inherently human as perceptual bias be embedded in code? \r\n        </p>\r\n        <p>\r\n        Nowadays, facial recognition systems rely on machine-learning frameworks to train complex algorithms by \r\n        feeding them large image datasets. Those images are generally categorized by age, gender, skin tone, and \r\n        a variety of other metrics. The more an algorithm is fed images of diverse faces, the more proficient it \r\n        becomes in identifying those faces. One possible explanation for the racial bias of facial recognition \r\n        systems may thus have to do with low public availability of diverse datasets – those are indeed quite rare. \r\n        The consequence is to encode the “other-race effect” into the software application layer in such a way that \r\n        algorithms ultimately become more apt to recognize faces that they have been exposed to/fed the most. \r\n        </p>\r\n        <p>\r\n        If indeed performance bias stems from a lack of representation in data, then what are companies and \r\n        organizations doing to address it? In January, IBM decided to publicly release a database of a million \r\n        faces that better reflect the variety of skin tones found in the real world. Named “Diversity in Faces,” \r\n        this massive dataset was meant “to advance the study of fairness and accuracy in facial recognition \r\n        technology.”  However, IBM’s initiative quickly backfired when NBC News journalist Olivia Solon revealed \r\n        that many of those images of ordinary people were collected from the photo-hosting site Flickr without their \r\n        consent. While IBM guaranteed users that they could opt out of the database, doing so proved very difficult. \r\n        Indeed, IBM required them to email links to the images they wanted removed, but never publicly shared the list \r\n        of Flickr users whose photos were included in the dataset. A year later, as Black Lives Matter protests \r\n        erupted all over the country following George Floyd’s death, IBM ultimately decided to abandon its facial \r\n        recognition research program, possibly implying that the only way to get rid of facial recognition bias was to \r\n        get rid of facial recognition altogether.\r\n        </p>\r\n        <p>\r\n        But not everyone gave up on improving the technology. In March of this year, a group of computer scientists and \r\n        engineers at the University of Durham proposed an innovative method to reduce bias occurrences without \r\n        infringing on privacy rights: per-subject adversarially-enabled data augmentation. What this does is leverage \r\n        the power of generative adversarial network (GAN) to transfer racial attributes of a human face while preserving \r\n        identity features. This technique allows to produce different versions of the same face across multiple race \r\n        groups, thus increasing the diversity of dataset images. However, some limitations must be noted. First, while \r\n        accuracy gains were made, they seemed minimal and only went up by 1%. GAN is a fairly new technology developed \r\n        in 2014 and may itself need further correction? Second, the race groups that this technology was tested on are \r\n        few: Caucasian, African, Indian, and Asian. A pressing question, then, is how to accommodate individuals who do \r\n        not fall within any of these groups (e.g. Pacific islanders) or whose traits are distributed across them. \r\n        </p>\r\n        <p>\r\n        Each of the solutions discussed above – IBM’s disavowal of facial recognition and adversarially-enabled data augmentation – their own merits and flaws.\r\n        </p>   \r\n        <p> Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor\r\n            incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud \r\n            exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure\r\n            dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur.\r\n            Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt \r\n            mollit anim id est laborum.\r\n        </p>\r\n        <p> Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor\r\n            incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud \r\n            exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure \r\n            dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. \r\n            Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt \r\n            mollit anim id est laborum. \r\n        </p> \r\n    </div>\r\n</div>\r\n    \r\n</> \r\n);\r\n\r\nexport default HomePage;"]},"metadata":{},"sourceType":"module"}