{"ast":null,"code":"import { jsxDEV as _jsxDEV } from \"react/jsx-dev-runtime\";\nimport { Fragment as _Fragment } from \"react/jsx-dev-runtime\";\nvar _jsxFileName = \"E:\\\\Software Development\\\\React\\\\my-blog\\\\src\\\\pages\\\\HomePage.jsx\";\nimport React from 'react';\nimport img from '../img/facial2.jpg';\n\nconst HomePage = () => /*#__PURE__*/_jsxDEV(_Fragment, {\n  children: /*#__PURE__*/_jsxDEV(\"div\", {\n    className: \"grid-container\",\n    children: [/*#__PURE__*/_jsxDEV(\"div\", {\n      className: \"cell1\",\n      children: /*#__PURE__*/_jsxDEV(\"div\", {\n        children: [\"\\\"I guess the computer got it wrong\\\"\", /*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n          fileName: _jsxFileName,\n          lineNumber: 8,\n          columnNumber: 49\n        }, this), \"Uncovering Racial Bias in Facial Recognition Technology\"]\n      }, void 0, true, {\n        fileName: _jsxFileName,\n        lineNumber: 8,\n        columnNumber: 9\n      }, this)\n    }, void 0, false, {\n      fileName: _jsxFileName,\n      lineNumber: 7,\n      columnNumber: 5\n    }, this), /*#__PURE__*/_jsxDEV(\"div\", {\n      className: \"cell2\",\n      children: /*#__PURE__*/_jsxDEV(\"img\", {\n        src: img,\n        alt: \"cannot display\",\n        width: \"100%\"\n      }, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 11,\n        columnNumber: 9\n      }, this)\n    }, void 0, false, {\n      fileName: _jsxFileName,\n      lineNumber: 10,\n      columnNumber: 5\n    }, this), /*#__PURE__*/_jsxDEV(\"div\", {\n      className: \"cell3\",\n      children: [/*#__PURE__*/_jsxDEV(\"p\", {\n        children: \"Earlier this year, Detroit police arrested Robert Williams, 42, on suspicion of stealing watches from a trendy Shinola retail store he had not patronized in years. Falsely identified as the suspect by Michigan State Police's face recognition software, Williams spent 30 hours in detention. Charges were dismissed once an investigator realized that \\u201Cthe computer got it wrong,\\u201D making this incident the first wrongful arrest in the U.S. due to a facial recognition mismatch.\"\n      }, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 14,\n        columnNumber: 9\n      }, this), /*#__PURE__*/_jsxDEV(\"p\", {\n        children: \"Since its early days, facial recognition has raised serious concerns among privacy advocates and ethics watchdogs who worry about data collection, processing and storage methods as well as unregulated use of the technology. As if it were not enough, William\\u2019s story brought to public attention a whole new \\u2013 and perhaps more radical \\u2013 set of questions: just how reliable is facial recognition? Can it be trusted at all? Long before his arrest, some scholars had already warned against varying error rates across demographic groups, which they viewed as a clear example of racial bias in facial biometric systems. In what follows, I will draw on corpus of tweets, newspaper articles, and scholarly essays to unpack this notion, identify its origins, and discuss recent efforts to correct it. I will be using the term \\u201Crace\\u201D instead of \\u201Cethnicity\\u201D to align with the terminology adopted in academic studies.\"\n      }, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 21,\n        columnNumber: 9\n      }, this), /*#__PURE__*/_jsxDEV(\"p\", {\n        children: \"Social psychologists agree that humans tend to perceive faces of their own race more acutely than those of other races. Referred to as \\u201Ccross-race effect\\u201D or \\u201Cother-race effect,\\u201D this phenomenon has been supported by a large corpus of evidence.  Most hypotheses suggest that the amount of exposure to and experience with a specific race group is correlated with the ability to recognize individuals from that group. This other-race effect partly explains how some systems struggle to identify people of color, especially women. According to the 2018 \\u201CGender Shades\\u201D project, algorithms developed by Microsoft, Amazon, and IBM yielded disconcerting results.  Darker-skinned women were 34% more likely to be misidentified than light-skinned males. This begs the question: how can something as inherently human as perceptual bias be embedded in code?\"\n      }, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 33,\n        columnNumber: 9\n      }, this), /*#__PURE__*/_jsxDEV(\"p\", {\n        children: \"Nowadays, facial recognition systems rely on machine-learning frameworks to train complex algorithms by feeding them large image datasets. Those images are generally categorized by age, gender, skin tone, and a variety of other metrics. The more an algorithm is fed diverse images, the more proficient it becomes in identifying those faces. What this means is that the racial bias in facial recognition technology often stems from a disproportionately low number of available diverse images \\u2013 something strangely reminiscent of the lack of exposure that defines the other-race effect.\"\n      }, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 44,\n        columnNumber: 9\n      }, this), /*#__PURE__*/_jsxDEV(\"p\", {\n        children: \" Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\"\n      }, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 52,\n        columnNumber: 9\n      }, this), /*#__PURE__*/_jsxDEV(\"p\", {\n        children: \" Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\"\n      }, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 59,\n        columnNumber: 9\n      }, this)]\n    }, void 0, true, {\n      fileName: _jsxFileName,\n      lineNumber: 13,\n      columnNumber: 5\n    }, this)]\n  }, void 0, true, {\n    fileName: _jsxFileName,\n    lineNumber: 6,\n    columnNumber: 1\n  }, this)\n}, void 0, false);\n\n_c = HomePage;\nexport default HomePage;\n\nvar _c;\n\n$RefreshReg$(_c, \"HomePage\");","map":{"version":3,"sources":["E:/Software Development/React/my-blog/src/pages/HomePage.jsx"],"names":["React","img","HomePage"],"mappings":";;;AAAA,OAAOA,KAAP,MAAkB,OAAlB;AACA,OAAOC,GAAP,MAAgB,oBAAhB;;AAEA,MAAMC,QAAQ,GAAG,mBACjB;AAAA,yBACA;AAAK,IAAA,SAAS,EAAC,gBAAf;AAAA,4BACI;AAAK,MAAA,SAAS,EAAC,OAAf;AAAA,6BACI;AAAA,yEAAwC;AAAA;AAAA;AAAA;AAAA,gBAAxC;AAAA;AAAA;AAAA;AAAA;AAAA;AADJ;AAAA;AAAA;AAAA;AAAA,YADJ,eAII;AAAK,MAAA,SAAS,EAAC,OAAf;AAAA,6BACI;AAAK,QAAA,GAAG,EAAED,GAAV;AAAe,QAAA,GAAG,EAAC,gBAAnB;AAAoC,QAAA,KAAK,EAAC;AAA1C;AAAA;AAAA;AAAA;AAAA;AADJ;AAAA;AAAA;AAAA;AAAA,YAJJ,eAOI;AAAK,MAAA,SAAS,EAAC,OAAf;AAAA,8BACI;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,cADJ,eAQI;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,cARJ,eAoBI;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,cApBJ,eA+BI;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,cA/BJ,eAuCI;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,cAvCJ,eA8CI;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,cA9CJ;AAAA;AAAA;AAAA;AAAA;AAAA,YAPJ;AAAA;AAAA;AAAA;AAAA;AAAA;AADA,iBADA;;KAAMC,Q;AAoEN,eAAeA,QAAf","sourcesContent":["import React from 'react';\r\nimport img from '../img/facial2.jpg';\r\n\r\nconst HomePage = () => ( \r\n<>\r\n<div className=\"grid-container\">\r\n    <div className=\"cell1\">\r\n        <div>\"I guess the computer got it wrong\"<br/>Uncovering Racial Bias in Facial Recognition Technology</div>\r\n    </div>\r\n    <div className=\"cell2\">\r\n        <img src={img} alt=\"cannot display\" width=\"100%\"/>  \r\n    </div>\r\n    <div className=\"cell3\">\r\n        <p>\r\n        Earlier this year, Detroit police arrested Robert Williams, 42, on suspicion of stealing watches \r\n        from a trendy Shinola retail store he had not patronized in years. Falsely identified as the suspect\r\n        by Michigan State Police's face recognition software, Williams spent 30 hours in detention. Charges \r\n        were dismissed once an investigator realized that “the computer got it wrong,” making this \r\n        incident the first wrongful arrest in the U.S. due to a facial recognition mismatch.\r\n        </p>\r\n        <p>\r\n        Since its early days, facial recognition has raised serious concerns among privacy advocates and \r\n        ethics watchdogs who worry about data collection, processing and storage methods as well as \r\n        unregulated use of the technology. As if it were not enough, William’s story brought to public attention a whole\r\n        new – and perhaps more radical – set of questions: just how reliable is facial recognition? Can it \r\n        be trusted at all? Long before his arrest, some scholars had already warned against varying \r\n        error rates across demographic groups, which they viewed as a clear example of racial bias in \r\n        facial biometric systems. In what follows, I will draw on corpus of tweets, newspaper articles, \r\n        and scholarly essays to unpack this notion, identify its origins, and discuss recent efforts to \r\n        correct it. I will be using the term “race” instead of “ethnicity” to align with the terminology \r\n        adopted in academic studies. \r\n        </p>\r\n        <p>\r\n        Social psychologists agree that humans tend to perceive faces of their own race more acutely than \r\n        those of other races. Referred to as “cross-race effect” or “other-race effect,” this phenomenon has \r\n        been supported by a large corpus of evidence.  Most hypotheses suggest that the amount of exposure to \r\n        and experience with a specific race group is correlated with the ability to recognize individuals from \r\n        that group. This other-race effect partly explains how some systems struggle to identify people of \r\n        color, especially women. According to the 2018 “Gender Shades” project, algorithms developed by \r\n        Microsoft, Amazon, and IBM yielded disconcerting results.  Darker-skinned women were 34% more likely \r\n        to be misidentified than light-skinned males. This begs the question: how can something as inherently \r\n        human as perceptual bias be embedded in code? \r\n        </p>\r\n        <p>\r\n        Nowadays, facial recognition systems rely on machine-learning frameworks to train complex algorithms \r\n        by feeding them large image datasets. Those images are generally categorized by age, gender, skin tone, \r\n        and a variety of other metrics. The more an algorithm is fed diverse images, the more proficient it \r\n        becomes in identifying those faces. What this means is that the racial bias in facial recognition \r\n        technology often stems from a disproportionately low number of available diverse images – something \r\n        strangely reminiscent of the lack of exposure that defines the other-race effect. \r\n        </p>\r\n        <p> Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor\r\n            incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud \r\n            exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure\r\n            dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur.\r\n            Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt \r\n            mollit anim id est laborum.\r\n        </p>\r\n        <p> Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor\r\n            incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud \r\n            exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure \r\n            dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. \r\n            Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt \r\n            mollit anim id est laborum. \r\n        </p> \r\n    </div>\r\n</div>\r\n    \r\n</> \r\n);\r\n\r\nexport default HomePage;"]},"metadata":{},"sourceType":"module"}